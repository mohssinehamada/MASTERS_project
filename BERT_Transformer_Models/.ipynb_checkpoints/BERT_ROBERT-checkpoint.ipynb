{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68621bc2-0fa9-4fd5-bc66-37ea32e2f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_train = pd.read_csv(\"train.tsv\", sep=\"\\t\", header=None)\n",
    "data_valid = pd.read_csv(\"valid.tsv\", sep=\"\\t\", header=None)\n",
    "data_test = pd.read_csv(\"test.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b2ba8-e077-4ebf-be26-8e95f342ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99824ab7-bac8-4d15-8943-08b5eed7dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e2913-ca0f-422f-9412-cafdb79c3eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac91b32e-63af-4bb7-9256-0586d4efb420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(dataset):\n",
    "    # Creating new 'label' column based on column 1\n",
    "    dataset['label'] = [1 if x in [\"true\", \"mostly-true\"] else 0 for x in dataset[1]]\n",
    "    \n",
    "    # Debug: print current columns\n",
    "    print(\"Columns before dropping:\", dataset.columns.tolist())\n",
    "    \n",
    "    # Drop unwanted columns by label (not by position)\n",
    "    dataset = dataset.drop(columns=[0, 1, 8, 9, 10, 11, 12])\n",
    "    \n",
    "    # Process metadata columns\n",
    "    meta = []\n",
    "    for i in range(len(dataset)):\n",
    "        subject = dataset.loc[i, 3] if dataset.loc[i, 3] != 0 else 'None'\n",
    "        speaker = dataset.loc[i, 4] if dataset.loc[i, 4] != 0 else 'None'\n",
    "        job = dataset.loc[i, 5] if dataset.loc[i, 5] != 0 else 'None'\n",
    "        state = dataset.loc[i, 6] if dataset.loc[i, 6] != 0 else 'None'\n",
    "        affiliation = dataset.loc[i, 7] if dataset.loc[i, 7] != 0 else 'None'\n",
    "        context = dataset.loc[i, 13] if dataset.loc[i, 13] != 0 else 'None'\n",
    "        meta.append(f\"{subject} {speaker} {job} {state} {affiliation} {context}\")\n",
    "    \n",
    "    # Add the combined metadata column\n",
    "    dataset['combined_meta'] = meta\n",
    "    # Create 'sentence' by combining metadata with text from column 2\n",
    "    dataset[\"sentence\"] = dataset['combined_meta'].astype(str) + \" \" + dataset[2].astype(str)\n",
    "    \n",
    "    # Now drop the original metadata columns and the temporary 'combined_meta'\n",
    "    dataset = dataset.drop(columns=[2, 3, 4, 5, 6, 7, 13, 'combined_meta'])\n",
    "    \n",
    "    # Drop any remaining rows with null values\n",
    "    dataset = dataset.dropna()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ddc74-da0b-41a7-bb98-8c177bd5157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_preprocessing(data_train)\n",
    "data_valid = data_preprocessing(data_valid)\n",
    "data_test = data_preprocessing(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8af852-db71-428d-931d-d47dd2c7a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308512a5-68f4-4af9-90d3-a9a59cc5700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d792369-a261-45d9-8335-717c7ef38038",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f710b-7929-409a-81cc-16ff19816e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330615c5-c819-4da0-88ca-2ecc975b4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d2c17-bc57-49eb-bd2d-317e4b5e3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353cf47b-1d7b-436d-a749-c6aeb6aad55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ca6ae-73ba-40e1-a33d-4e1b8027ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = [] \n",
    "for sent in data_train[\"sentence\"]:\n",
    "    sent_len.append(len(sent))\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig = plt.figure(figsize =(10, 7))\n",
    "plt.boxplot(sent_len)\n",
    "plt.show()\n",
    "\n",
    "sent_len = [i for i in sent_len if i<=500] #Excluding the outliers\n",
    "fig2 = plt.figure(figsize =(10, 7))\n",
    "plt.hist(sent_len, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6c74a-a986-4c9c-a11a-44d4402a2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d4d6f8-14db-442c-ac5c-799b34dc8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ba194-a6e9-4290-835f-0ee804933327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "872f0bdb-c2eb-4dd3-a95a-0ccc0a95f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d181916-65f1-4d3c-8a17-584d862adbff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d28c76-d63b-4dc1-ae4e-72bb6965de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40704987-cf33-49be-a2f2-b43351c2b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2a0ca-9315-455f-8e41-3926e9990f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())   # Should be False on Mac\n",
    "print(torch.backends.mps.is_available())  # True if using an M1/M2 Mac with proper PyTorch support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e520a-41d5-4cfe-a8f3-668c7634f30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71057060-347a-413a-84f3-477b2390717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", #Using BERT base model with an uncased vocab.\n",
    "                                                                num_labels = 2, #number of output labels - 0,1 (binary classification)\n",
    "                                                                output_attentions = False, #model doesnt return attention weights\n",
    "                                                                output_hidden_states = False #model doesnt return hidden states\n",
    "                                                          )\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "\n",
    "bert_model = bert_model.to(\"mps\")\n",
    "\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", #RoBERTa base model\n",
    "                                                                    num_labels = 2,  #number of output labels - 0,1 (binary classification)\n",
    "                                                                    output_attentions = False,  #model doesnt return attention weights\n",
    "                                                                    output_hidden_states = False #model doesnt return hidden states\n",
    "                                                                )\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n",
    "\n",
    "bert_model = bert_model.to(\"mps\")\n",
    "print(\"base models loaded \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eddfd21-e1ce-434b-b5bf-ae6b1c5aa510",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "bert_model = bert_model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb739d50-c4ad-40a2-97f0-1f3c7db0c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' Original: ', data_train[\"sentence\"][0])\n",
    "\n",
    "# Split the sentence into tokens - BERT\n",
    "print('Tokenized BERT: ', bert_tokenizer.tokenize(data_train[\"sentence\"][0]))\n",
    "\n",
    "# Mapping tokens to token IDs - BERT\n",
    "print('Token IDs BERT: ', bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(data_train[\"sentence\"][0])))\n",
    "\n",
    "# Split the sentence into tokens -RoBERTa\n",
    "print('Tokenized RoBERT: ', roberta_tokenizer.tokenize(data_train[\"sentence\"][0]))\n",
    "\n",
    "# Mapping tokens to token IDs - RoBERTa\n",
    "print('Token IDs RoBERTa: ', roberta_tokenizer.convert_tokens_to_ids(roberta_tokenizer.tokenize(data_train[\"sentence\"][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a5ec105-b20e-4f25-939c-0fbca7f3a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data_train[\"sentence\"].values \n",
    "labels = data_train[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e320b33-9b00-44fa-9006-6c2fd1ae6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5bbfe5-4ffb-4b6b-8178-f438edf67e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_robert_tokenization(dataset):\n",
    "  sentences = dataset[\"sentence\"].values\n",
    "  labels = dataset[\"label\"].values\n",
    "  max_length = 256\n",
    "\n",
    "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "  bert_input_ids = []\n",
    "  bert_attention_masks = []\n",
    "  roberta_input_ids = []\n",
    "  roberta_attention_masks = []\n",
    "\n",
    "  sentence_ids = []\n",
    "  counter = 0\n",
    "  for sent in sentences:\n",
    "      bert_encoded_dict = bert_tokenizer.encode_plus(\n",
    "          str(sent),\n",
    "          add_special_tokens=True,\n",
    "          max_length=256,\n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = True , \n",
    "            truncation = True, \n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        roberta_encoded_dict = roberta_tokenizer.encode_plus(\n",
    "            str(sent),\n",
    "            add_special_tokens = True,\n",
    "            max_length = 256,\n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = True , \n",
    "            truncation = True , \n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "        #Add the encoded sentence to the List \n",
    "        bert_input.ids.append(bert_encoded_dict[\"input_ids\"])\n",
    "        roberta_input_ids.append(roberta_encoded_dict[\"input_idc\"])\n",
    "\n",
    "        #Add attemtion mask to the list \n",
    "        bert_attention_maks.append(bert_encoded_dict[\"attention_mask\"])\n",
    "        roberta_attention_maks.append(roberta_encoded_dict[\"attention_mask\"])\n",
    "        #collecting sentence_ids \n",
    "        sentence_ids.append(counter)\n",
    "        counter = counter + 1\n",
    "#Convert the list into tensors \n",
    "bert_input_ids = torch.cat(bert_input_ids , dim = 0)\n",
    "bert_attention_masks = torch.cat(bert_attention_maks, dim  = 0 )\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c0bad-a56a-4281-ad07-18ce88aa278d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc3c05-e3a7-4722-addc-d6407905428a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4cbe44-50bf-46e2-ae49-37e0e4fe879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset , random_split \n",
    "torch.manual_seed(0)\n",
    "\n",
    "token_dict_train = bert_robert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde78584-cb17-4b30-8845-4157241abef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898b6dd-e52a-43d4-aea6-0a5248778cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d9b699-661f-4a7d-9913-273730aafd61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2536ba6e-247a-443f-8b9b-c375f831cf23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b6eaa-a800-47b0-884f-d09d82b0b2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a333c3-f4e0-4f77-b47c-6b8f6aef9e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb0078-2e18-47a4-8e1c-3d846dee064c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa97de7-9bc3-4717-bb76-5ee33dc3f2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858372b0-9711-43e1-8025-2c668604d89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ab90f-afcc-4427-af0d-e7c8fbfe82d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcc5e1-6dd6-49f6-8f67-4e089ad4f5c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4425d-289b-43b0-bece-f878deb0e8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923a99f-adfb-4207-9018-784ff4301f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722d144-e5ff-43f0-8584-6f2fbe24579e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e4664-ac5e-430c-aa6b-fe3fd2deb943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834a609-f24a-4dee-8e1d-052ced1e1acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b8470-9ad2-4c60-934e-a6807dc170bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15253fe2-75e4-46e3-a262-cc7d52e61f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae32b9-b3b8-4e27-a13a-e3d8f0a0b5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
